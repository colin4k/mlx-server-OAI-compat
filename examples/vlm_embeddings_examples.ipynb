{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision-Language Model (VLM) Embeddings with MLX Server\n",
    "\n",
    "This notebook demonstrates how to leverage the embeddings endpoint of MLX Server through its OpenAI-compatible API. Vision-Language Models (VLMs) can process both images and text, allowing for multimodal understanding and representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "MLX Server provides an efficient way to serve multimodal models on Apple Silicon. In this notebook, we'll explore how to:\n",
    "\n",
    "- Generate embeddings for text and images\n",
    "- Work with the OpenAI-compatible API\n",
    "- Calculate similarity between text and image representations\n",
    "- Understand how these embeddings can be used for practical applications\n",
    "\n",
    "Embeddings are high-dimensional vector representations of content that capture semantic meaning, making them useful for search, recommendation systems, and other AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and API Connection\n",
    "\n",
    "- A local server endpoint (`http://localhost:8000/v1`)\n",
    "- A placeholder API key (since MLX Server doesn't require authentication by default)\n",
    "\n",
    "Make sure you have MLX Server running locally before executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI client for API communication\n",
    "from openai import OpenAI\n",
    "\n",
    "# Connect to the local MLX Server with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"fake-api-key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Processing for API Requests\n",
    "\n",
    "When working with image inputs, we need to prepare them in a format that the API can understand. The OpenAI-compatible API expects images to be provided as base64-encoded data URIs.\n",
    "\n",
    "Below, we'll import the necessary libraries and define a helper function to convert PIL Image objects to the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To send images to the API, we need to convert them to base64-encoded strings in a data URI format.\n",
    "\n",
    "def image_to_base64(image: Image.Image):\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to a base64-encoded data URI string that can be sent to the API.\n",
    "    \n",
    "    Args:\n",
    "        image: A PIL Image object\n",
    "        \n",
    "    Returns:\n",
    "        A data URI string with the base64-encoded image\n",
    "    \"\"\"\n",
    "    # Convert image to bytes\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)\n",
    "    image_data = buffer.getvalue()\n",
    "    \n",
    "    # Encode as base64\n",
    "    image_base64 = base64.b64encode(image_data).decode('utf-8')\n",
    "    \n",
    "    # Create the data URI format required by the API\n",
    "    mime_type = \"image/png\"  \n",
    "    image_uri = f\"data:{mime_type};base64,{image_base64}\"\n",
    "    \n",
    "    return image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Preparing an Image\n",
    "\n",
    "Now we'll load a sample image (a green dog in this case) and convert it to the base64 format required by the API. This image will be used to generate embeddings in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"images/green_dog.jpeg\")\n",
    "image_uri = image_to_base64(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding for a single text input\n",
    "prompt = \"Describe the image in detail\"\n",
    "image_embedding = client.embeddings.create(\n",
    "    input=[prompt],\n",
    "    model=\"mlx-community/Qwen2.5-VL-3B-Instruct-4bit\",\n",
    "    extra_body = {\n",
    "        \"image_url\": image_uri\n",
    "    }\n",
    ").data[0].embedding\n",
    "\n",
    "text = \"A green dog looking at the camera\"\n",
    "text_embedding = client.embeddings.create(\n",
    "    input=[text],\n",
    "    model=\"mlx-community/Qwen2.5-VL-3B-Instruct-4bit\"\n",
    ").data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Text and Image Embeddings\n",
    "\n",
    "One of the powerful features of VLM embeddings is that they create a shared vector space for both text and images. This means we can directly compare how similar a text description is to an image's content by calculating the cosine similarity between their embeddings.\n",
    "\n",
    "A higher similarity score (closer to 1.0) indicates that the text description closely matches the image content according to the model's understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8473370724651375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b)\n",
    "\n",
    "similarity = cosine_similarity(image_embedding, text_embedding)\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
